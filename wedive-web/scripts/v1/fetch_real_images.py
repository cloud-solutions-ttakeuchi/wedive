import json
import os
import requests
import time

# è¨­å®š
INPUT_FILE = "src/data/creatures_seed.json"   # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆç”Ÿç‰©ãƒªã‚¹ãƒˆã®æ­£ï¼‰
OUTPUT_FILE = "src/data/creatures_real.json"  # ç”»åƒå–å¾—çµæœã®ä¿å­˜å…ˆ
SAVE_INTERVAL = 10  # ä¸­é–“ä¿å­˜ã®é–“éš”

def fetch_wiki_image(query, lang='ja'):
    """Wikipedia APIã‹ã‚‰ç”»åƒã‚’å–å¾—"""
    base_url = f"https://{lang}.wikipedia.org/w/api.php"
    headers = { "User-Agent": "DiveDexBot/1.0 (contact@example.com)" }
    params = {
        "action": "query", "format": "json", "prop": "pageimages|pageterms",
        "piprop": "original", "titles": query, "pithumbsize": 500, "redirects": 1
    }
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=10)
        if response.status_code != 200: return None
        data = response.json()
        pages = data.get("query", {}).get("pages", {})
        for page_id, page in pages.items():
            if page_id == "-1" or "original" not in page: continue
            return {
                "url": page["original"]["source"],
                "credit": f"Wikipedia ({lang})",
                "license": "CC BY-SA"
            }
    except Exception:
        pass
    return None

def load_and_merge_data():
    """
    seedï¼ˆå…¨ãƒªã‚¹ãƒˆï¼‰ã¨realï¼ˆå–å¾—æ¸ˆã¿ç”»åƒï¼‰ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹é‡è¦ãƒ­ã‚¸ãƒƒã‚¯
    """
    # 1. seedï¼ˆæœ€æ–°ã®ç”Ÿç‰©ãƒªã‚¹ãƒˆï¼‰ãŒãªã„å ´åˆã¯å§‹ã¾ã‚‰ãªã„
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ Seed file not found: {INPUT_FILE}")
        return []

    print(f"ğŸ“‚ Loading seed data from {INPUT_FILE}...")
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        seed_creatures = json.load(f)

    # 2. realï¼ˆéå»ã«å–å¾—ã—ãŸç”»åƒãƒ‡ãƒ¼ã‚¿ï¼‰ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€
    real_image_map = {}
    if os.path.exists(OUTPUT_FILE):
        print(f"ğŸ“‚ Loading existing images from {OUTPUT_FILE}...")
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                real_creatures = json.load(f)
                # åå‰ã‚’ã‚­ãƒ¼ã«ã—ã¦ã€æœ‰åŠ¹ãªç”»åƒãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’ãƒãƒƒãƒ—ã«ä¿æŒ
                for c in real_creatures:
                    img = c.get("image", "")
                    # æœ¬ç‰©ã®ç”»åƒï¼ˆloremflickrä»¥å¤–ï¼‰ã‚’æŒã£ã¦ã„ã‚‹å ´åˆã®ã¿è¨˜æ†¶
                    if img.startswith("http") and "loremflickr" not in img:
                        real_image_map[c["name"]] = {
                            "image": c["image"],
                            "imageCredit": c.get("imageCredit"),
                            "imageLicense": c.get("imageLicense")
                        }
        except json.JSONDecodeError:
            pass

    # 3. seedãƒªã‚¹ãƒˆã«ã€realã®ç”»åƒã‚’é©ç”¨ï¼ˆãƒãƒ¼ã‚¸ï¼‰
    merged_count = 0
    for c in seed_creatures:
        name = c["name"]
        if name in real_image_map:
            # æ—¢ã«ç”»åƒå–å¾—æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ã€seedã®æƒ…å ±ã‚’ä¸Šæ›¸ã
            c.update(real_image_map[name])
            merged_count += 1

    print(f"   -> Merged existing images for {merged_count} creatures.")
    return seed_creatures

def save_data(data):
    """ä¿å­˜å‡¦ç†"""
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(" ğŸ’¾ Saved progress.")

def main():
    # ãƒãƒ¼ã‚¸æ¸ˆã¿ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆã“ã‚Œã§æ–°è¦è¿½åŠ åˆ†ã‚‚ã€å–å¾—æ¸ˆã¿åˆ†ã‚‚æ­£ã—ã„çŠ¶æ…‹ã«ãªã‚‹ï¼‰
    creatures = load_and_merge_data()
    if not creatures:
        return

    print(f"Checking images for {len(creatures)} creatures...")

    updated_count = 0
    skipped_count = 0

    for i, creature in enumerate(creatures):
        name = creature.get("name", "Unknown")
        current_image = creature.get("image", "")

        # --- ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š ---
        # ãƒãƒ¼ã‚¸æ¸ˆã¿ãªã®ã§ã€ã“ã“ã§ã®ãƒã‚§ãƒƒã‚¯ã§ã€Œéå»ã«å–å¾—æ¸ˆã¿ã€ã‹ã©ã†ã‹ã‚ã‹ã‚‹
        if current_image.startswith("http") and "loremflickr" not in current_image:
             skipped_count += 1
             continue
        # ------------------

        en_keyword = creature.get("imageKeyword", "")
        print(f"[{i+1}/{len(creatures)}] ğŸ” Searching: {name}...", end="", flush=True)

        # å„ªå…ˆé †ä½: å­¦å(en) -> å’Œå(ja) -> è‹±å(en) -> ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(en)
        scientific_name = creature.get("scientificName")
        english_name = creature.get("englishName")

        # 1. å­¦åã§æ¤œç´¢ (æœ€ã‚‚ç¢ºå®Ÿ)
        if scientific_name:
            # print(f" (Scientific: {scientific_name})...", end="", flush=True)
            image_data = fetch_wiki_image(scientific_name, 'en') # å­¦åã¯è‹±èªWikipediaã§ãƒ’ãƒƒãƒˆã—ã‚„ã™ã„

        # 2. å’Œåã§æ¤œç´¢
        if not image_data:
            image_data = fetch_wiki_image(name, 'ja')

        # 3. è‹±åã§æ¤œç´¢ (ã‚¹ãƒšãƒ¼ã‚¹ã‚ã‚Šã®æ­£å¼å)
        if not image_data and english_name:
             # print(f" (English: {english_name})...", end="", flush=True)
             image_data = fetch_wiki_image(english_name, 'en')

        # 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ (æœ€å¾Œã®æ‰‹æ®µ)
        if not image_data and en_keyword and en_keyword != english_name:
            print(f" (Keyword: {en_keyword})...", end="", flush=True)
            image_data = fetch_wiki_image(en_keyword, 'en')

        if image_data:
            creature["image"] = image_data["url"]
            creature["imageCredit"] = image_data["credit"]
            creature["imageLicense"] = image_data["license"]
            print(" âœ… Found!")
            updated_count += 1
        else:
            print(" âŒ Not found")

        # ä¸­é–“ä¿å­˜
        if updated_count > 0 and updated_count % SAVE_INTERVAL == 0:
            save_data(creatures)

        time.sleep(1.0)

    # æœ€çµ‚ä¿å­˜
    save_data(creatures)

    print(f"\nâœ¨ Done!")
    print(f"   - Total checked: {len(creatures)}")
    print(f"   - Newly Fetched: {updated_count}")
    print(f"   - Skipped (Already done): {skipped_count}")
    print(f"   - Saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
