# Immediate startup log (bypassing logging config to ensure it's seen)
print("üöÄ Starting Cleansing Pipeline Script...", flush=True)

import json
import os
import argparse
import time
import logging
from typing import List, Dict, Any
from datetime import datetime, timezone
from google import genai
from google.genai import types
import firebase_admin
from firebase_admin import credentials, firestore
import sys

# --- Logging Configuration ---
log_level = os.environ.get("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, log_level, logging.INFO),
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# --- Configuration ---
PROJECT_ID = (
    os.environ.get("GOOGLE_CLOUD_PROJECT") or
    os.environ.get("GCLOUD_PROJECT") or
    os.environ.get("VITE_FIREBASE_PROJECT_ID")
)
LOCATION = os.environ.get("LOCATION") or os.environ.get("AI_AGENT_LOCATION")

if not PROJECT_ID:
    print("‚ùå FATAL: PROJECT_ID is not set. Checked: GOOGLE_CLOUD_PROJECT, GCLOUD_PROJECT, VITE_FIREBASE_PROJECT_ID", flush=True)
    sys.exit(1)

if not LOCATION:
    print("‚ùå FATAL: LOCATION is not set. Checked: LOCATION, AI_AGENT_LOCATION", flush=True)
    sys.exit(1)

print(f"‚ÑπÔ∏è Configured with PROJECT_ID={PROJECT_ID}, LOCATION={LOCATION}, LOG_LEVEL={log_level}", flush=True)

class CleansingPipeline:
    def __init__(self):
        self.model_name = "gemini-2.0-flash-001"
        # Use us-central1 as default for AI if not specified,
        # as gemini-2.0-flash and caching are more likely to be available there.
        self.ai_location = os.environ.get("AI_LOCATION") or "us-central1"
        self.project_id = PROJECT_ID

        logger.info(f"ü§ñ Initializing GenAI Client (AI_LOCATION={self.ai_location})")
        self.client = genai.Client(
            vertexai=True,
            project=self.project_id,
            location=self.ai_location
        )
        self.cache = None

        # Initialize Firestore
        if not firebase_admin._apps:
            firebase_admin.initialize_app(options={'projectId': PROJECT_ID})
        self.db = firestore.client()

    def load_data(self, filters: Dict[str, Any]):
        """Fetch points and creatures from Firestore based on hierarchy-aware filters."""
        logger.info("üì° Fetching data from Firestore...")

        # 1. Load Creatures (All for context cache)
        creatures_ref = self.db.collection('creatures')
        self.creatures = [doc.to_dict() | {"id": doc.id} for doc in creatures_ref.stream()]

        # 2. Load hierarchy-aware Master data if needed (small collections)
        areas_dict = {doc.id: doc.to_dict() for doc in self.db.collection('areas').stream()}
        zones_dict = {doc.id: doc.to_dict() for doc in self.db.collection('zones').stream()}

        # 3. Load Points and filter in-memory for robustness
        points_ref = self.db.collection('points')
        if filters.get('pointId'):
            # Specific point: direct access
            doc = points_ref.document(filters['pointId']).get()
            raw_points = [doc.to_dict() | {"id": doc.id}] if doc.exists else []
        else:
            # Load all and filter hierarchically
            raw_points = [doc.to_dict() | {"id": doc.id} for doc in points_ref.stream()]

        self.points = []
        for p in raw_points:
            # Higher-level filters check
            if filters.get('area') and p.get('areaId') != filters['area']:
                continue

            if filters.get('zone'):
                area = areas_dict.get(p.get('areaId'))
                if not area or area.get('zoneId') != filters['zone']:
                    continue

            if filters.get('region'):
                area = areas_dict.get(p.get('areaId'))
                if area:
                    zone = zones_dict.get(area.get('zoneId'))
                    if not zone or zone.get('regionId') != filters['region']:
                        continue
                else:
                    continue

            self.points.append(p)

        logger.info(f"üìä Loaded {len(self.creatures)} creatures and {len(self.points)} target points.")
        logger.info(f"üîé Applied Filters: {json.dumps(filters, indent=2)}")

    def create_context_cache(self):
        """Creates a context cache for biological data to save token costs."""
        logger.info("üíæ Creating Context Cache for Biological Dictionary...")
        system_instruction = (
            "„ÅÇ„Å™„Åü„ÅØÊµ∑Ê¥ãÁîüÁâ©Â≠¶ËÄÖ„Åß„Åô„ÄÇÊèê‰æõ„Åï„Çå„ÅüÁîüÁâ©„É™„Çπ„Éà„ÅÆÁîüÊÖã„Å´Âü∫„Å•„Åç„ÄÅ"
            "ÁâπÂÆö„ÅÆ„ÉÄ„Ç§„Éì„É≥„Ç∞„Éù„Ç§„É≥„Éà„Å´ÁîüÊÅØ„Åó„Å¶„ÅÑ„Çã„ÅãÂà§ÂÆö„Åó„Åæ„Åô„ÄÇÂá∫Âäõ„ÅØÂøÖ„ÅöJSONÂΩ¢Âºè„Åß„ÄÇ"
        )

        creatures_context = "\n".join([
            f"ID:{c['id']} - {c['name']}: {c.get('description', '')} (Ê∞¥Ê∑±:{json.dumps(c.get('depthRange'))})"
            for c in self.creatures
        ])

        try:
            self.cache = self.client.caches.create(
                model=self.model_name,
                config=types.CreateCachedContentConfig(
                    display_name=f"bio_cache_{datetime.now(timezone.utc).strftime('%Y%m%d%H')}",
                    system_instruction=system_instruction,
                    contents=[creatures_context],
                    ttl="86400s",
                )
            )
            logger.info(f"‚úÖ Context Cache created: {self.cache.name}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Context Caching not available or failed: {e}. Proceeding without cache (higher token cost).")
            self.cache = None

    def cleanup_cache(self):
        """Deletes the context cache to free up resources immediately."""
        if self.cache:
            try:
                logger.info(f"üßπ Deleting Context Cache: {self.cache.name}")
                self.client.caches.delete(name=self.cache.name)
                self.cache = None
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to delete cache: {e}")

    def _safe_json_parse(self, text: str):
        """Clean and parse JSON from AI response."""
        try:
            # Remove Markdown code blocks if present
            clean_text = text.strip()
            if clean_text.startswith("```"):
                lines = clean_text.splitlines()
                if lines[0].startswith("```json"):
                    clean_text = "\n".join(lines[1:-1])
                else:
                    clean_text = "\n".join(lines[1:-1])

            return json.loads(clean_text)
        except Exception as e:
            logger.error(f"Failed to parse JSON: {e}\nRaw Text: {text}")
            return None

    def run_stage1_batch(self, point) -> List[Dict[str, Any]]:
        """Stage 1: Batch physical constraint filtering via Cache."""
        response_schema = {
            "type": "ARRAY",
            "items": {
                "type": "OBJECT",
                "properties": {
                    "creature_id": {"type": "STRING"},
                    "is_possible": {"type": "BOOLEAN"},
                    "rarity": {"type": "STRING"},
                    "confidence": {"type": "NUMBER"},
                    "reasoning": {"type": "STRING"}
                },
                "required": ["creature_id", "is_possible", "rarity", "confidence", "reasoning"]
            }
        }

        # Incorporate filters into instructions to focus the AI
        filter_instr = ""
        if point.get('specific_creature_name'):
             filter_instr = f"- ‰ªäÂõû„ÅÆÂà§ÂÆöÂØæË±°„ÅØ„Äå{point['specific_creature_name']}„Äç1Á®ÆÈ°û„ÅÆ„Åø„Åß„Åô„ÄÇ‰ªñ„ÅÆÁîüÁâ©„ÅØ‰∏ÄÂàá„É™„Çπ„Éà„Å´Âê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ"
        else:
             filter_instr = "- „Éù„Ç§„É≥„Éà„ÅÆÁí∞Â¢É„Å´ÂêàËá¥„Åô„ÇãÁîüÁâ©„Çí„É™„Çπ„Éà„Åã„ÇâÊºè„Çå„Å™„ÅèÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"

        prompt = f"""
        „ÉÄ„Ç§„Éì„É≥„Ç∞„Éù„Ç§„É≥„Éà„Äå{point['name']}„Äç„ÅÆÊù°‰ª∂„Å´Âü∫„Å•„Åç„ÄÅÂØæË±°ÁîüÁâ©„ÅåÁîüÊÅØÂèØËÉΩ„Åã„ÄÅ„Ç≠„É£„ÉÉ„Ç∑„É•ÂÜÖ„ÅÆËæûÊõ∏„Åã„ÇâÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        „Éù„Ç§„É≥„ÉàÊ∞¥Ê∑±: {point.get('maxDepth', 40)}m
        Âú∞ÂΩ¢: {json.dumps(point.get('topography', []))}

        „ÄêÊåáÁ§∫„Äë
        {filter_instr}
        - ÁîüÊÅØÂèØËÉΩÔºàis_possible=trueÔºâ„Å™Â†¥Âêà„Å´JSON„Å´Âê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        - ÊúüÂæÖ„Åï„Çå„ÇãÂ∏åÂ∞ëÂ∫¶(rarity)„ÄÅÁ¢∫‰ø°Â∫¶(confidence: 0.0-1.0)„ÄÅÁêÜÁî±(reasoning)„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        - Âá∫ÂäõÂΩ¢Âºè„ÅØÁ¥îÁ≤ã„Å™JSONÈÖçÂàó„ÅÆ„Åø„Å®„Åó„ÄÅË™¨ÊòéÊñá„Å™„Å©„ÅØ‰∏ÄÂàáÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
        """

        logger.debug(f"Stage 1 Prompt for {point['name']}: {prompt}")
        try:
            config = types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=response_schema,
            )
            # Use cache only if available
            if self.cache:
                config.cached_content = self.cache.name

            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=config
            )
            text = getattr(response, 'text', '') or response.candidates[0].content.parts[0].text
            result = self._safe_json_parse(text)
            return result if isinstance(result, list) else []
        except Exception as e:
            # Check if it is a cache expiration error
            error_msg = str(e)
            if "expired" in error_msg.lower() and self.cache:
                logger.warning(f"‚ö†Ô∏è Cache expired during processing. Re-creating cache to maintain cost efficiency...")
                self.create_context_cache()
                # Retry with the newly created cache (if creation succeeded)
                return self.run_stage1_batch(point)

            logger.warning(f"‚ö†Ô∏è Stage 1 Error for {point['name']}: {e}")
            return []

    def run_stage2_grounding(self, point, creature) -> Dict[str, Any]:
        """Stage 2: Factual verification via Google Search Tool."""
        response_schema = {
            "type": "OBJECT",
            "properties": {
                "actual_existence": {"type": "BOOLEAN"},
                "evidence": {"type": "STRING"},
                "rarity": {"type": "STRING"}
            },
            "required": ["actual_existence", "evidence", "rarity"]
        }

        prompt = f"""
        „Äå{point['name']}„ÄçÔºà{point.get('region','')}, {point.get('area','')}Ôºâ„Å´„Åä„ÅÑ„Å¶„ÄÅ
        ÁîüÁâ©„Äå{creature['name']}„Äç„ÅÆÁõÆÊíÉÂÆüÁ∏æ„ÇÑÁîüÊÅØÊÉÖÂ†±„ÇíGoogleÊ§úÁ¥¢„ÅßÁ≤æÊüª„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        ÂõûÁ≠î„ÅØÂøÖ„ÅöÊåáÂÆö„Åï„Çå„ÅüJSONÂΩ¢Âºè„Åß„ÄÇ
        """

        logger.debug(f"Stage 2 Prompt for {creature['name']}: {prompt}")
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    tools=[types.Tool(google_search=types.GoogleSearch())],
                    response_mime_type="application/json",
                    response_schema=response_schema,
                )
            )
            text = getattr(response, 'text', '') or response.candidates[0].content.parts[0].text
            result = self._safe_json_parse(text)
            return result if isinstance(result, dict) else {"actual_existence": False, "evidence": "Parse Error", "rarity": "Unknown"}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Stage 2 Error for {creature['name']}: {e}")
            return {"actual_existence": False, "evidence": str(e), "rarity": "Unknown"}

    def process(self, mode: str, filters: Dict[str, Any], limit: int):
        processed_count = 0
        try:
            self.load_data(filters)
            self.create_context_cache()

            for p in self.points:
                if processed_count >= limit: break
                logger.info(f"üîé Processing Point: {p['name']} ({p['id']})")

                # Add target creature name to point info for Stage 1 focus
                p['specific_creature_name'] = None
                if filters.get('creatureId'):
                    creature = next((c for c in self.creatures if c['id'] == filters['creatureId']), None)
                    if creature:
                        p['specific_creature_name'] = creature['name']

                s1_results = self.run_stage1_batch(p)

                for res in s1_results:
                    if processed_count >= limit: break
                    creature_id = res.get("creature_id")
                    if not res.get("is_possible") or not creature_id: continue

                    # Optional: pinpoint creature filter
                    if filters.get('creatureId') and creature_id != filters['creatureId']:
                        continue

                    key = f"{p['id']}_{creature_id}"

                    # Check existence in Firestore if mode is 'new'
                    if mode == "new":
                        existing = self.db.collection('point_creatures').document(key).get()
                        if existing.exists:
                            logger.debug(f"  ‚è≠Ô∏è Skipping existing: {creature_id}")
                            continue

                    creature = next((c for c in self.creatures if c['id'] == creature_id), None)
                    if not creature: continue

                    # Stage 2: Fact-check with Grounding (Only if Stage 1 is unsure)
                    if res.get("confidence", 0) >= 0.85:
                        logger.info(f"  ‚ú® AI is confident ({res.get('confidence')}). Saving without search.")
                        s2 = {
                            "actual_existence": True,
                            "evidence": res.get("reasoning"),
                            "rarity": res.get("rarity")
                        }
                    else:
                        logger.info(f"  üåê AI is unsure. Running Google Search Grounding: {creature['name']}...")
                        s2 = self.run_stage2_grounding(p, creature)

                    # Save result to Firestore
                    status = "pending" if s2.get("actual_existence") else "rejected"

                    # Final check: if rejected but high confidence in S1, maybe it's just 'Rare'
                    if status == "rejected" and res.get("confidence", 0) > 0.8:
                        logger.info(f"  üí° High confidence S1 result kept as 'pending' despite no web evidence.")
                        status = "pending"

                    new_entry = {
                        "pointId": p['id'],
                        "creatureId": creature['id'],
                        "localRarity": s2.get("rarity") or res.get("rarity") or "Rare",
                        "status": status,
                        "reasoning": s2.get("evidence") or res.get("reasoning"),
                        "confidence": (res.get("confidence", 0.5) + (0.3 if s2.get("actual_existence") else 0)) / 1.3,
                        "updatedAt": firestore.SERVER_TIMESTAMP,
                        "method": "python-batch-v1"
                    }

                    self.db.collection('point_creatures').document(key).set(new_entry)
                    logger.info(f"  üöÄ Saved: {creature['name']} -> {status}")
                    processed_count += 1

                    # Small sleep to be nice to API quotas (adjust as needed)
                    time.sleep(0.5)

        finally:
            self.cleanup_cache()

        logger.info(f"üèÅ Finished. Processed {processed_count} mappings.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WeDive AI Cleansing Pipeline (Bulk / Specific)")
    parser.add_argument("--mode", choices=["all", "new", "specific", "replace"], default="new", help="all: full scan, new: skip existing, specific: targeted scan, replace: overwrite specific")
    parser.add_argument("--limit", type=int, default=100000, help="Maximum number of mappings to process")
    parser.add_argument("--pointId", help="Target a specific point")
    parser.add_argument("--creatureId", help="Target a specific creature")
    parser.add_argument("--region", help="Filter points by region")
    parser.add_argument("--zone", help="Filter points by zone")
    parser.add_argument("--area", help="Filter points by area")

    args = parser.parse_args()

    filters = {
        "pointId": args.pointId,
        "creatureId": args.creatureId,
        "region": args.region,
        "zone": args.zone,
        "area": args.area
    }

    pipeline = CleansingPipeline()
    pipeline.process(mode=args.mode, filters=filters, limit=args.limit)
