import os
import json
import time
import difflib
import google.generativeai as genai
import argparse
import shutil
from typing import List, Dict, Set

# --- è¨­å®š ---ã€€APIKEYã€€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šå¯
API_KEYS = os.environ.get("GOOGLE_API_KEY", "").split(",")
if not API_KEYS or not API_KEYS[0]:
    raise ValueError("GOOGLE_API_KEY environment variable is not set.")
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CONFIG_DIR = os.path.join(BASE_DIR, "scripts/config")
DATA_DIR = os.path.join(BASE_DIR, "src/data")
INPUT_FILE = os.path.join(CONFIG_DIR, "target_areas.json")
OUTPUT_FILE = os.path.join(DATA_DIR, "locations_seed.json")

# é‡è¤‡åˆ¤å®šã®é–¾å€¤
SIMILARITY_THRESHOLD = 0.85

SCHEMA_PROMPT = """
å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã®JSONé…åˆ—ï¼ˆArray of Objectsï¼‰ã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
Markdownã®ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆã¯ä¸è¦ã§ã™ã€‚

Object Schema:
[
  {
    "name": "Point Name (e.g. é’ã®æ´çªŸ)",
    "type": "Point",
    "level": "Beginner / Intermediate / Advanced",
    "maxDepth": int (meter),
    "entryType": "boat / beach",
    "current": "none / weak / strong / drift",
    "topography": ["cave", "dropoff", "sand", "rock" ...],
    "features": ["ç‰¹å¾´ã‚¿ã‚°1", "ç‰¹å¾´ã‚¿ã‚°2"],
    "latitude": float (e.g. 26.4),
    "longitude": float (e.g. 127.8),
    "description": "ãƒã‚¤ãƒ³ãƒˆã®é­…åŠ›ã‚„ç‰¹å¾´ã‚’100æ–‡å­—ç¨‹åº¦ã§ã€‚",
    "imageKeyword": "ç”»åƒæ¤œç´¢ç”¨è‹±å˜èª (e.g. blue cave okinawa)"
  }
]
"""

def is_similar(name1: str, name2: str) -> bool:
    """æ–‡å­—åˆ—ã®é¡ä¼¼åº¦åˆ¤å®š (Levenshtein-like)"""
    matcher = difflib.SequenceMatcher(None, name1, name2)
    return matcher.ratio() >= SIMILARITY_THRESHOLD

def check_duplicate(new_point_name: str, existing_names: Set[str]) -> str:
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
    if new_point_name in existing_names: return new_point_name
    for existing in existing_names:
        if is_similar(new_point_name, existing):
            return existing
    return None

    return None

def get_existing_point_names(data: List[Dict]) -> Set[str]:
    names = set()
    for region in data:
        for zone in region.get("children", []):
            for area in zone.get("children", []):
                for point in area.get("children", []):
                    names.add(point["name"])
    return names

# Models to cycle through
CANDIDATE_MODELS = [
    'gemini-2.5-flash',
    'gemini-2.5-flash-lite',
    'gemma-3-27b-it',
    'gemma-3-12b-it',
    'gemma-3-4b-it',
    'gemma-3-2b-it',
    'gemma-3-1b-it',
]

# Flattened Resource Pool: [(model, key), (model, key)...]
RESOURCE_POOL = [(m, k) for m in CANDIDATE_MODELS for k in API_KEYS]
current_resource_index = 0

def get_current_resource():
    return RESOURCE_POOL[current_resource_index]

def rotate_resource():
    global current_resource_index
    current_resource_index = (current_resource_index + 1) % len(RESOURCE_POOL)
    print(f"    ğŸ”„ Switching to Resource #{current_resource_index + 1}/{len(RESOURCE_POOL)}")

def generate_points(region: str, zone: str, area: str) -> List[Dict]:
    global current_resource_index

    prompt = f"""
    ã‚ãªãŸã¯ãƒ™ãƒ†ãƒ©ãƒ³ã®ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚
    æŒ‡å®šã•ã‚ŒãŸã€ŒAreaï¼ˆã‚¨ãƒªã‚¢ï¼‰ã€ã«ã‚ã‚‹ã€å€‹åˆ¥ã®ã€ŒPointï¼ˆãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã‚¹ãƒãƒƒãƒˆï¼‰ã€ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚

    Region: {region}
    Zone: {zone}
    Area: {area}

    å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONï¼‰:
    [
      {{
        "name": "Pointåï¼ˆä¾‹: ãƒãƒ³ã‚¿ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«, åŒ—ã®æ ¹ï¼‰",
        "desc": "ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å¾´ã€è¦‹ã‚‰ã‚Œã‚‹ç”Ÿç‰©ã€æ°´æ·±ã€æµã‚Œãªã©ã‚’150æ–‡å­—ä»¥å†…ã§",
        "latitude": ç·¯åº¦(æ•°å€¤),
        "longitude": çµŒåº¦(æ•°å€¤)
      }}
    ]

    æ³¨æ„ç‚¹:
    - å…·ä½“çš„ã§å®Ÿåœ¨ã™ã‚‹ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ãƒã‚¤ãƒ³ãƒˆã‚’3ã€œ6å€‹ç¨‹åº¦ã€‚
    - Pointåã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆã€ŒåŒ—ã®æ ¹ã€ãªã©ã¯ã‚¨ãƒªã‚¢åã‚’å† ã™ã‚‹ãªã©åŒºåˆ¥ã§ãã‚‹ã‚ˆã†ã«ï¼‰ã€‚
    - ç·¯åº¦çµŒåº¦ã¯æ¦‚ç®—ã§æ§‹ã„ã¾ã›ã‚“ã€‚
    - ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
    """

    max_attempts = len(RESOURCE_POOL)
    attempts = 0

    while attempts < max_attempts:
        model_name, api_key = get_current_resource()

        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)

            response = model.generate_content(prompt)
            text = response.text.strip()
            # Remove markdown if present
            if text.startswith("```json"):
                text = text[7:]
            if text.endswith("```"):
                text = text[:-3]

            result = json.loads(text)
            if result:
                # Success! Keep the current index as is (it's working).
                # Identify which key index this matches for display (just for info)
                key_display_idx = API_KEYS.index(api_key) + 1
                print(f"    âœ… Success with {model_name} (Key #{key_display_idx})")
                return result

        except Exception as e:
            error_str = str(e)
            if "429" in error_str:
                print(f"    âš ï¸ Quota exceeded: {model_name} (Key index in pool: {current_resource_index})")
                rotate_resource()
                time.sleep(1)
            elif "404" in error_str or "not found" in error_str.lower():
                print(f"    â„¹ï¸ Model {model_name} not found/supported. Skipping.")
                rotate_resource()
            else:
                print(f"    âŒ Error with {model_name}: {e}")
                rotate_resource()

        attempts += 1

    print(f"    ğŸ’€ All resources failed for {area}")
    return []

def main():
    parser = argparse.ArgumentParser(description="Generate Points data.")
    parser.add_argument("--mode", choices=["append", "overwrite", "clean"], default="append",
                        help="Execution mode: append (skip existing), overwrite (replace existing), clean (start fresh)")
    args = parser.parse_args()

    if not os.path.exists(INPUT_FILE):
        print(f"âŒ Config file not found: {INPUT_FILE}")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        target_areas = json.load(f)

    all_locations = []

    # Mode: Clean
    if args.mode == "clean":
        if os.path.exists(OUTPUT_FILE):
            shutil.copy(OUTPUT_FILE, OUTPUT_FILE + ".bak")
            print(f"ğŸ“¦ Backed up existing file to {OUTPUT_FILE}.bak")
        all_locations = []
    # Mode: Append / Overwrite
    elif os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            try:
                all_locations = json.load(f)
            except:
                pass

    # å…¨é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã‚»ãƒƒãƒˆä½œæˆ
    global_existing_points = get_existing_point_names(all_locations)
    print(f"â„¹ï¸  Existing unique points: {len(global_existing_points)}")

    print(f"ğŸš€ Generating Points for {len(target_areas)} areas... [Mode: {args.mode.upper()}]")

    for target in target_areas:
        region_name = target["region"]
        zone_name = target["zone"]
        area_name = target["area"]

        print(f"  Processing {region_name} > {zone_name} > {area_name}...")

        # Area Nodeæ¤œç´¢
        region_node = next((r for r in all_locations if r["name"] == region_name), None)
        if not region_node: continue
        zone_node = next((z for z in region_node.get("children", []) if z["name"] == zone_name), None)
        if not zone_node: continue
        area_node = next((a for a in zone_node.get("children", []) if a["name"] == area_name), None)
        if not area_node:
            print(f"    âš ï¸ Area {area_name} not found. Skipping.")
            continue

        existing_points = area_node.get("children", [])

        # Mode: Append - Skip if points exist
        if args.mode == "append" and len(existing_points) > 0:
             print(f"    â­ï¸  Skipping (Points already exist).")
             continue

        # Mode: Overwrite - Clear existing points
        if args.mode == "overwrite" and len(existing_points) > 0:
             print(f"    â™»ï¸  Overwriting points...")
             # Remove removed points from global tracker to allow recreation if names match
             for p in existing_points:
                 if p["name"] in global_existing_points:
                     global_existing_points.remove(p["name"])
             existing_points = []

        new_points = generate_points(region_name, zone_name, area_name)

        for new_p in new_points:
            sim_name = check_duplicate(new_p["name"], global_existing_points)

            if sim_name:
                print(f"    âš ï¸ SKIPPING: '{new_p['name']}' (Similar to '{sim_name}')")
            else:
                new_p["id"] = f"p_{int(time.time())}_{new_p['name']}"
                new_p["image"] = ""
                existing_points.append(new_p)
                global_existing_points.add(new_p["name"])
                print(f"    + Added Point: {new_p['name']}")

        area_node["children"] = existing_points

        # Save Incrementally
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_locations, f, indent=2, ensure_ascii=False)
        print(f"    ğŸ’¾ Progress saved to {OUTPUT_FILE}")

        time.sleep(2)

    print(f"\nâœ… All Done!")

if __name__ == "__main__":
    main()
