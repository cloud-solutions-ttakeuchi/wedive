import os
import json
import time
import math
import hashlib
import google.generativeai as genai
from typing import List, Dict

# --- è¨­å®š ---
# --- è¨­å®š ---
# API Key Handling
API_KEYS = os.environ.get("GOOGLE_API_KEY", "").split(",")
if not API_KEYS or not API_KEYS[0]:
    raise ValueError("GOOGLE_API_KEY environment variable is not set.")

BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CONFIG_DIR = os.path.join(BASE_DIR, "scripts/config")
DATA_DIR = os.path.join(BASE_DIR, "src/data")
TARGET_FAMILIES_FILE = os.path.join(CONFIG_DIR, "target_families.json")
OUTPUT_FILE = os.path.join(DATA_DIR, "creatures_seed.json")

BATCH_SIZE = 5
COUNT_PER_GROUP = 10

SCHEMA_PROMPT = """
å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒ(JSON Array):
[
  {
    "name": "ç”Ÿç‰©å(å’Œå)",
    "englishName": "English Name",
    "scientificName": "Scientific Name",
    "family": "ç§‘å(å’Œå)",
    "category": "é­šé¡ | ã‚¦ãƒŸã‚¦ã‚· | ç”²æ®»é¡ | ã‚µãƒ³ã‚´ | ãã®ä»– | å¤§ç‰©",
    "description": "50æ–‡å­—ç¨‹åº¦ã®è§£èª¬",
    "imageKeyword": "ç”»åƒæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
    "tags": ["ç‰¹å¾´ã‚¿ã‚°1", "ç‰¹å¾´ã‚¿ã‚°2", "è‰²", "æ¨¡æ§˜"],
    "rarity": "Common | Rare | Epic | Legendary (ä¸€èˆ¬çš„ãªãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã§ã®é­é‡é›£æ˜“åº¦)",
    "size": "ã‚µã‚¤ã‚ºç›®å®‰ (ä¾‹: 15cm, 1.5m)",
    "depthRange": {
      "min": æœ€å°æ°´æ·±(æ•°å€¤),
      "max": æœ€å¤§æ°´æ·±(æ•°å€¤)
    },
    "stats": {
      "rarity": 1-100 (æ•°å€¤),
      "popularity": 1-100 (äººæ°—åº¦),
      "danger": 0-100 (å±é™ºåº¦ æ¯’ã‚„æ”»æ’ƒæ€§),
      "size": 1-100 (ã‚µã‚¤ã‚ºæ„Ÿ),
      "speed": 1-100 (æ³³ãé€Ÿã•),
      "lifespan": 1-100 (å¯¿å‘½ã‚¤ãƒ¡ãƒ¼ã‚¸)
    },
    "waterTempRange": {
        "min": 20,
        "max": 30
    },
    "specialAttributes": ["æ¯’", "æ“¬æ…‹", "å…±ç”Ÿ", "å¤œè¡Œæ€§", "å›ºæœ‰ç¨®", "è¢«å†™ä½“", "ç¾ã—ã„", "ã‹ã‚ã„ã„", "ç¾¤ã‚Œ", "å¤§ç‰©", "å›éŠé­š"] ã®ä¸­ã‹ã‚‰è©²å½“ã™ã‚‹ã‚‚ã®,
    "season": ["æ˜¥", "å¤", "ç§‹", "å†¬"] ã®ä¸­ã‹ã‚‰è¦‹ã‚‰ã‚Œã‚‹å­£ç¯€ï¼ˆè¤‡æ•°å¯ï¼‰
  }
]
"""
# --- Class Definitions for Robust API Handling ---

class APIResource:
    def __init__(self, api_key: str, model_name: str, priority: int):
        self.api_key = api_key
        self.model_name = model_name
        self.priority = priority
        self.status = 'stand-by' # 'stand-by' | 'active' | 'stop'
        self.quota_exceed_dt = 0.0

RESOURCE_POOL: List[APIResource] = []

# Initialize Pool
# Priority: Flash > Flash-Lite
for key in API_KEYS:
    if not key: continue
    RESOURCE_POOL.append(APIResource(key, 'gemini-2.5-flash', 1))

for key in API_KEYS:
    if not key: continue
    RESOURCE_POOL.append(APIResource(key, 'gemini-2.5-flash-lite', 2))

def get_best_resource() -> APIResource:
    """Design: Priority & Status based selection"""
    current_time = time.time()

    # 1. Check for release from 'stop' state
    for r in RESOURCE_POOL:
        if r.status == 'stop':
            if current_time - r.quota_exceed_dt > 65:
                r.status = 'stand-by'
                r.quota_exceed_dt = 0.0

    # 2. Select 'stand-by' with highest priority
    candidates = [r for r in RESOURCE_POOL if r.status == 'stand-by']
    if candidates:
        candidates.sort(key=lambda x: x.priority)
        best = candidates[0]
        best.status = 'active'
        return best

    return None

def _call_gemini_api(target: str, count: int) -> List[Dict]:
    """Gemini APIã‚’å©ã"""

    prompt = f"""
    ã‚ãªãŸã¯æµ·æ´‹ç”Ÿç‰©å­¦è€…ã§ã™ã€‚
    ãƒ€ã‚¤ãƒãƒ¼ã«äººæ°—ã®é«˜ã„ã€Œ{target}ã€ã®ä»²é–“ã‚’ {count} ç¨®é¡ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚

    æ¡ä»¶:
    1. ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã§è¦‹ã‚‰ã‚Œã‚‹ç¨®ã‚’ä¸­å¿ƒã«é¸å®šã™ã‚‹ã“ã¨ã€‚
    2. å­¦å(scientificName)ã¯æ­£ç¢ºã«è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚
    3. JSONä»¥å¤–ã®æ–‡å­—åˆ—ã¯å‡ºåŠ›ã—ãªã„ã“ã¨ã€‚
    4. stat, tags, description, depthRange, waterTempRange, specialAttributes ãªã©å…¨ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç¶²ç¾…çš„ã«ç”Ÿæˆã™ã‚‹ã“ã¨ã€‚
    5. specialAttributesã¯ã€é…åˆ—å†…ã®ãƒ—ãƒªã‚»ãƒƒãƒˆå€¤ã‹ã‚‰é©åˆ‡ãªã‚‚ã®ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚

    {SCHEMA_PROMPT}
    """

    while True:
        resource = get_best_resource()

        if not resource:
            stopped_resources = [r for r in RESOURCE_POOL if r.status == 'stop']
            if not stopped_resources:
                print("    âŒ All resources invalid/stopped but no timer set. Aborting.")
                return []

            earliest_release = min(r.quota_exceed_dt for r in stopped_resources) + 65
            wait_seconds = earliest_release - time.time()

            if wait_seconds > 0:
                print(f"    â³ All resources exhausted. Waiting {wait_seconds:.1f}s for rate limit release...")
                time.sleep(wait_seconds + 1)
                continue
            else:
                time.sleep(1)
                continue

        try:
            # Execute Request
            genai.configure(api_key=resource.api_key)
            model = genai.GenerativeModel(resource.model_name)

            response = model.generate_content(prompt)
            text = response.text.strip()
            # Remove markdown if present
            if text.startswith("```json"): text = text[7:]
            if text.startswith("```"): text = text[3:]
            if text.endswith("```"): text = text[:-3]
            if text.strip().endswith("}"): text += "]"

            data = json.loads(text)
            result = data if isinstance(data, list) else [data]
            if result:
                # Success
                key_display_idx = API_KEYS.index(resource.api_key) + 1
                print(f"    âœ… Success with {resource.model_name} (Key #{key_display_idx})")

                resource.status = 'stand-by'

                # ğŸ›‘ RATE LIMIT HANDLING: Wait 5 seconds after success
                time.sleep(5)
                return result

        except Exception as e:
            error_str = str(e)
            if "429" in error_str:
                print(f"    âš ï¸ Quota exceeded (429): {resource.model_name} (Key ends {resource.api_key[-4:]})")
                # print(f"    DEBUG Info: {error_str[:200]}...") # Uncomment to see full error details if needed

                resource.status = 'stop'
                resource.quota_exceed_dt = time.time()

                # PREVIOUSLY: We stopped all models with this key.
                # CHANGE: Only stop THIS specific model/key combo to allow fallback to Lite (which has separate RPM).
                # for r in RESOURCE_POOL:
                #    if r.api_key == resource.api_key:
                #        r.status = 'stop'
                #        r.quota_exceed_dt = time.time()

            elif "404" in error_str or "not found" in error_str.lower():
                print(f"    â„¹ï¸ Model {resource.model_name} not found. Removing from pool.")
                if resource in RESOURCE_POOL:
                    RESOURCE_POOL.remove(resource)
            else:
                print(f"    âŒ Error with {resource.model_name}: {e}")
                resource.status = 'stand-by'
                time.sleep(1)

def generate_creatures_by_group(target: str, total_count: int) -> List[Dict]:
    """ãƒãƒƒãƒå‡¦ç†ã§ç”Ÿæˆ"""
    print(f"Generating {total_count} creatures for group: {target}...")
    combined_data = []
    num_batches = math.ceil(total_count / BATCH_SIZE)

    for i in range(num_batches):
        current_count = min(BATCH_SIZE, total_count - len(combined_data))
        batch_data = _call_gemini_api(target, current_count)

        if batch_data:
            for item in batch_data:
                # IDç”Ÿæˆ (å­¦åã‚’å„ªå…ˆã‚­ãƒ¼ã¨ã™ã‚‹)
                seed_str = item.get("scientificName") or item.get("name")
                unique_hash = hashlib.sha256(seed_str.encode()).hexdigest()[:16]
                item["id"] = f"c_{unique_hash}"
                item["image"] = "" # ç”»åƒã¯åˆ¥é€”å–å¾—

            combined_data.extend(batch_data)
            print(f"    -> Batch {i+1}/{num_batches}: Got {len(batch_data)} items.")
        else:
            print(f"    -> Batch {i+1}/{num_batches}: Failed.")

        time.sleep(2)

    return combined_data

import argparse
import shutil

# ... (imports remain)
import os
import json
import time
import math
import hashlib
import google.generativeai as genai
from typing import List, Dict

# ... (rest of imports/constants up to main)

def main():
    parser = argparse.ArgumentParser(description="Generate creature data based on taxonomy.")
    parser.add_argument("--mode", choices=["append", "overwrite", "clean"], default="append",
                        help="Generation mode: append (default), overwrite, or clean.")
    args = parser.parse_args()

    if not API_KEYS:
        print("âš ï¸ API Key missing.")
        return

    # Clean mode: Backup and delete existing file
    if args.mode == "clean":
        if os.path.exists(OUTPUT_FILE):
            timestamp = int(time.time())
            backup_path = f"{OUTPUT_FILE}.{timestamp}.bak"
            shutil.move(OUTPUT_FILE, backup_path)
            print(f"ğŸ§¹ Clean mode: Existing file backed up to {backup_path}")
        else:
            print("ğŸ§¹ Clean mode: No existing file to backup.")

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ (å­¦åã§åå¯„ã›ç”¨ãƒãƒƒãƒ—ä½œæˆ)
    all_creatures = []
    scientific_map = {}

    if os.path.exists(OUTPUT_FILE) and args.mode != "clean":
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                all_creatures = json.load(f)
                for c in all_creatures:
                    if "scientificName" in c:
                        scientific_map[c["scientificName"]] = c
        except Exception as e:
            print(f"âš ï¸ Error loading existing file: {e}")
            pass

    print(f"ğŸ“‚ Loaded {len(all_creatures)} creatures. Mode: {args.mode}")

    added_count = 0
    updated_count = 0
    skipped_count = 0

    # ç”Ÿæˆå¯¾è±¡ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿
    target_groups = []
    if os.path.exists(TARGET_FAMILIES_FILE):
        with open(TARGET_FAMILIES_FILE, 'r', encoding='utf-8') as f:
            target_groups = json.load(f)
        print(f"ğŸ“– Loaded {len(target_groups)} target families from file.")
    else:
        print(f"âš ï¸ Target families file not found: {TARGET_FAMILIES_FILE}")
        return

    # Resume Logic: Load processed log
    PROCESSED_LOG = os.path.join(CONFIG_DIR, "processed_families_log.json")
    processed_groups = set()
    if os.path.exists(PROCESSED_LOG) and args.mode == "append":
        try:
            with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:
                processed_groups = set(json.load(f))
            print(f"ğŸ”„ Resuming... Skipping {len(processed_groups)} already processed families.")
        except:
            pass
    elif args.mode == "clean":
        # Clear log on clean
        if os.path.exists(PROCESSED_LOG):
            os.remove(PROCESSED_LOG)

    for group in target_groups:
        if args.mode == "append" and group in processed_groups:
             print(f"    â­ï¸  Skipping {group} (Already processed).")
             continue

        # Check if we need to generate for this group at all (optimization for append)
        # Note: Since we generate by group, checking individual items happens after fetching,
        # or we could try to skip the whole group if we knew it was fully populated.
        # For now, we'll fetch and then filter/merge.

        new_items = generate_creatures_by_group(group, COUNT_PER_GROUP)

        # Mark as processed ONLY if we successfully got items.
        # If new_items is empty (e.g. due to API errors), do NOT mark as processed so we can retry.
        if new_items and args.mode == "append":
            processed_groups.add(group)
            with open(PROCESSED_LOG, 'w', encoding='utf-8') as f:
                json.dump(list(processed_groups), f, indent=2, ensure_ascii=False)

        for item in new_items:
            s_name = item.get("scientificName")

            if s_name and s_name in scientific_map:
                existing = scientific_map[s_name]

                if args.mode == "overwrite":
                    # For safety in this context (creatures linked by ID), let's UPDATE content.
                    # CRITICAL FIX: Do NOT overwrite ID or Image if they exist
                    safe_update_item = item.copy()
                    if "id" in safe_update_item: del safe_update_item["id"]
                    if "image" in safe_update_item: del safe_update_item["image"]
                    if "imageUrl" in safe_update_item: del safe_update_item["imageUrl"] # Just in case

                    existing.update(safe_update_item) # item has new data (description, etc)

                    updated_count += 1

                elif args.mode == "append":
                    # Append: Skip
                    skipped_count += 1
                    continue
            else:
                # New item
                all_creatures.append(item)
                if s_name: scientific_map[s_name] = item
                added_count += 1

        # Save validation (file update) per group to prevent data loss on crash
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_creatures, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Done! Added: {added_count}, Updated/Overwritten: {updated_count}, Skipped: {skipped_count}")

if __name__ == "__main__":
    main()
